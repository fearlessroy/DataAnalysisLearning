{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不重复的词： ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "每个单词的 ID： {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "每个单词的 tfidf 值： [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n",
      "  0.33040189 0.         0.40412895]\n",
      " [0.         0.         0.40412895 0.40412895 0.         0.63314609\n",
      "  0.33040189 0.         0.40412895]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.         0.52210862 0.52210862 0.         0.\n",
      "  0.42685801 0.         0.52210862]]\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec=TfidfVectorizer()\n",
    "\n",
    "#创建文档列表 documents\n",
    "documents=[\n",
    "    'this is the bayes document',\n",
    "    'this is the second document',\n",
    "    'and the third one',\n",
    "    'is this the document'\n",
    "]\n",
    "# 使用 tfidf_vec 对 documents 进行拟合，得到 TF-IDF 矩阵\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n",
    "\n",
    "print('不重复的词：',tfidf_vec.get_feature_names())\n",
    "print('每个单词的 ID：',tfidf_vec.vocabulary_)\n",
    "print('每个单词的 tfidf 值：',tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/wangyunfei/Desktop/text_classification-master/text classification/stop/stopword.txt','r') as f:\n",
    "#     stop_words=[line.strip() for line in f.readlines()]\n",
    "stop_words = open('./text_classification-master/text classification/stop/stopword.txt', 'r', encoding='utf-8').read()\n",
    "stop_words = stop_words.encode('utf-8').decode('utf-8-sig') # 列表头部\\ufeff处理\n",
    "stop_words = stop_words.split('\\n') # 根据分隔符分隔\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对文本分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_words(file_path):\n",
    "    '''\n",
    "    对文本进行分词\n",
    "    :return: 用空格进行分词的字符串\n",
    "    '''\n",
    "    texit_with_space=''\n",
    "    text=open(file_path,'r',encoding='gb18030').read()\n",
    "    text_cut=jieba.cut(text)\n",
    "    for word in text_cut:\n",
    "        texit_with_space += word + ''\n",
    "    return texit_with_space\n",
    "\n",
    "def load_file(file_dir,label):\n",
    "    '''\n",
    "    将路径下的所有文件加载\n",
    "    :return: 分词后的文档列表和标签\n",
    "    '''\n",
    "    file_list = os.listdir(file_dir)\n",
    "    words_list = []\n",
    "    lables_list = []\n",
    "    for file in file_list:\n",
    "        file_path = file_dir+'/'+file\n",
    "        words_list.append(cut_words(file_path))\n",
    "        lables_list.append(label)\n",
    "    return words_list,lables_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_list1,train_labels1=load_file('./text_classification-master/text classification/train/女性','女性')\n",
    "train_words_list2,train_labels2=load_file('./text_classification-master/text classification/train/体育','体育')\n",
    "train_words_list3,train_labels3=load_file('./text_classification-master/text classification/train/文学','文学')\n",
    "train_words_list4,train_labels4=load_file('./text_classification-master/text classification/train/校园','校园')\n",
    "\n",
    "train_words_list=train_words_list1+train_words_list2+train_words_list3+train_words_list4\n",
    "train_labels = train_labels1+train_labels2+train_labels3+train_labels4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words_list1,test_labels1=load_file('./text_classification-master/text classification/test/女性','女性')\n",
    "test_words_list2,test_labels2=load_file('./text_classification-master/text classification/test/体育','体育')\n",
    "test_words_list3,test_labels3=load_file('./text_classification-master/text classification/test/文学','文学')\n",
    "test_words_list4,test_labels4=load_file('./text_classification-master/text classification/test/校园','校园')\n",
    "\n",
    "test_words_list=test_words_list1+test_words_list2+test_words_list3+test_words_list4\n",
    "test_labels = test_labels1+test_labels2+test_labels3+test_labels4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算单词权重，直接创建 TfidfVectorizer 类，然后使用 fit_transform 进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。        \n",
    "max_df 参数用来描述单词在文档中的最高出现率，假设 max_df =0.5，代表单词在 50% 的文档中都出现过，那么它只携带了非常少的信息，因此就不作为分词统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words=stop_words,max_df=0.5)\n",
    "\n",
    "train_features=tf.fit_transform(train_words_list)\n",
    "test_features=tf.transform(test_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项式贝叶斯分类器， alpha为平滑参数，假如一个单词在训练样本中没有出现，这个单词的概率就会被计算为0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。了解了这个问题，我们需要做平滑处理。\n",
    "当 alpha = 1，就是采用 Laplace 平滑，Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率问题。            \n",
    "当 0<alpha<1，采用的是 Lidstone 平滑，对于 Lidstone 平滑来说，alpha 越小，迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为： 0.765\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.001).fit(train_features,train_labels)\n",
    "\n",
    "predicted_labels = clf.predict(test_features)\n",
    "\n",
    "# 计算准确率\n",
    "print(\"准确率为：\",metrics.accuracy_score(test_labels,predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
